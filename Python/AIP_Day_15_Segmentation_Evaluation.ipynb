{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=\"center\">Advanced Image Processing and Analysis</h1>\n",
    "<h3 align=\"center\">ECE 4438B/ECE 9022/ECE 9202B/BIOMED 9519B/BIOPHYS 9519B/CAMI 9519B</h3>\n",
    "<h4 align=\"center\"><a href=\"mailto:echen29@uwo.ca?subject=Day 15: Segmentation Evaluation Lecture\"> Elvis Chen, PhD, LL</a></h4>\n",
    "<h4 align=\"center\">Day 15, March 04, 2019</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import SimpleITK as sitk\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "%run update_path_to_download_script\n",
    "from downloaddata import fetch_data as fdata\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from ipywidgets import interact, fixed\n",
    "from myshow import myshow, myshow3d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Similarity Metrics in Detail\n",
    "\n",
    "Let us revisit the Similarity metrics in more detail, as they will be used again when we are learning about image registration (and the evaluation of).  Define the following sets:\n",
    "\n",
    "* $I$ be the image,\n",
    "* $S$, or the *Source*, be a segmented region as the output of an algorithm to be evaluated, and\n",
    "* $T$, or the *Target*, be a reference segmentation of which we are using as a gold standard.\n",
    "\n",
    "Generally speacking, $S$ and $T$ are not identical:\n",
    "<img src=\"VennDiag.png\" width=\"650\"/>\n",
    "\n",
    "Using $T$ as the reference, we can further define the following:\n",
    "* The intersection: $ S \\bigcap T$. This is the set of pixels labelled as the foreground by the algorithm, and the reference data agrees with it.  In other words, $S \\bigcap T$ is the **True Positive (TP)**,\n",
    "* The Union: $S \\bigcup T$. This is the set of pixels labelled as the foreground **either** by the algorithm **or** by the reference segmentation,\n",
    "  * Conversely, $I - S \\bigcap T$ is the set of the pixel labelled as the background by our algorithm, and the reference data agrees with it.  That is: $I - S \\bigcap T$ is the **True Negative**.\n",
    "* The Set Difference of $S$ and $T$: $S \\setminus T$. This is the set of pixels labelled as the foreground by the algorithm but as background by the reference segmentation. That is, $S \\setminus T$ is the **False Positive**, and\n",
    "* The Set Difference of $T$ and $S$: $T \\setminus S$. This is the set of pixels labelled as the background by the algorithm but as foreground by the reference segmentation. That is, $T \\setminus S$ is the **False Negative**.\n",
    "\n",
    "<img src=\"ConfusionMatrix.PNG\" width=\"650\"/>\n",
    "\n",
    "Another way to visualize these relation is the **Confusion Table**:\n",
    "<img src=\"ConfusionMatrixTable.PNG\" width=\"650\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using these notation, we can further define the following metrics:\n",
    "\n",
    "[Jaccard similarity coefficient](https://en.wikipedia.org/wiki/Jaccard_index), AKA **Intersection over Union**\n",
    "\n",
    "\n",
    "$J(S,T) = \\frac{| S \\bigcap T |}{| S \\bigcup T |} = \\frac{| S \\bigcap T |}{ | S | + | T | - | S | \\bigcap | T |}$ \n",
    "\n",
    "and\n",
    "\n",
    "[Dice Similarit Cofficient](https://en.wikipedia.org/wiki/S%C3%B8rensen%E2%80%93Dice_coefficient) is another area based measure of similarity:\n",
    "\n",
    "$DSC(S,T) = \\frac{ 2 | S \\bigcap T | }{ | S | + | T | }$\n",
    "\n",
    "where the operator $| |$ denotes the pixel counts within the set. Both the Jaccard and Dice metrics have a range between $[0,1]$. In fact, they are monotonic to each other: Jaccard metric is always larger than Dice metric except at the extrema $\\{0,1\\}$ where they are equal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sensitivity and Specificity\n",
    "\n",
    "While Dice metric (and by extension, Jaccard index) measures the amount of the overlapping between the segmentated result and what we assumed to be the good answer (**note**: I am still not using the word *ground truth*), it does not tell us anything about the *shape* or the edges of the segmented region compared to the good answer:\n",
    "\n",
    "<img src=\"OverUnder.PNG\" width=\"550\"/>\n",
    "\n",
    "The image pair shown below is a case of *over-segmentation* (left) versus a different algorithm that did not suffer from the over-segmentation problem.\n",
    "\n",
    "![An example of over segmentation](https://www.researchgate.net/profile/Ashraf_Aly_H/publication/267753061/figure/fig4/AS:295575825207311@1447482176812/Example-image-from-grayscale-images-a-Over-segmentation-problem-by-using-Active-Contour.png)\n",
    "\n",
    "Two other metrics often used is the **True Positive Rate (TPR)** (also known as *sensitivity* and *recall*), and the **True Negative Rate (TNR)** (also known as Specificity), define as:\n",
    "\n",
    "**True Positive Rate (TPR)** (Sensitivity/Recall), \n",
    "\n",
    "$\\frac{TP}{TP+FN}$ or $\\frac{ | S \\bigcap T |}{ | T |}$\n",
    "\n",
    "and\n",
    "**True Negative Rate (TPR)** (Specificity)\n",
    "\n",
    "$\\frac{TN}{TN+FP}$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To understand what Sensitivity (TPR) and Specificity (TNR) mean, imaging you have a car alarm:\n",
    "* True Positive (TP) means the alarm is sound and there is a break-in,\n",
    "* False Positive (FP) means the alarm is sound but there is NO break-in,\n",
    "* False Negative (FN) means the alarm is NOT sound but there IS a break-in, and\n",
    "* True Negative (TN) means the alarm is not sound and there is no break-in.\n",
    "\n",
    "Obviously having high TP and high TN is good, high FP and hight FN is not desired.\n",
    "\n",
    "Supose the sensitivity of the proximity sensor can be adjusted. If it is VERY sensitive:\n",
    "* True Positive goes up: you catch all the break-in, BUT\n",
    "* False Positive goes up as well (cars driving by will trigger the alarm),\n",
    "* False Negative goes down,\n",
    "* True Negative does down.\n",
    "\n",
    "Thus, **Sensitivity**, or True Positive Rate, is the extent to which true foreground pixels are correctly identified (so false negatives are few).  **If the segmentation is sensitive, then it is good at identifying the foreground pixels.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specificity, on the other hand, relates to how good a segmentation at identify the background pixels. It is the proportion of known background pixtures (True Negative) to those evaluated to be background. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Questions: What are the sensitivity/specificity for the over/under segmentation cases?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question: What does it mean that an segmentation is both sensitive and specific?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load an image: CT of a head\n",
    "img_ct = sitk.ReadImage(fdata(\"cthead1.png\"))\n",
    "\n",
    "# to visualize the labels image in RGB needs an image with 0-255 range\n",
    "#\n",
    "# we don't need to do this for THIS particular image (it is already rescaled).\n",
    "img_255 = sitk.Cast(sitk.RescaleIntensity(img_ct), sitk.sitkUInt8)\n",
    "\n",
    "myshow(img_ct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = (100, 170)\n",
    "seg_connectedThreshold = sitk.Image(img_ct.GetSize(), sitk.sitkUInt8)\n",
    "seg_connectedThreshold.CopyInformation(img_ct)\n",
    "seg_connectedThreshold[seed] = 1\n",
    "seg_connectedThreshold = sitk.BinaryDilate(seg_connectedThreshold, 3)\n",
    "myshow(sitk.LabelOverlay(img_255, seg_connectedThreshold), \"Initial Seed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seg_connectedThreshold = sitk.ConnectedThreshold(img_ct, seedList=[seed], lower=230, upper=255)\n",
    "myshow(sitk.LabelOverlay(img_255,seg_connectedThreshold), \"Connected Threshold\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seg_confidenceConnected = sitk.ConfidenceConnected(img_ct, \n",
    "                                                   seedList=[seed], \n",
    "                                                   numberOfIterations = 10, \n",
    "                                                   multiplier = 2.5, \n",
    "                                                   initialNeighborhoodRadius = 3, \n",
    "                                                   replaceValue = 1)\n",
    "myshow(sitk.LabelOverlay( img_255, seg_confidenceConnected), \"Conficence Connected Threshold\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visually, the result of the ConfidenceConnected segmentation is an *under* segmentation of the ConnectedThreshold technique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Jaccard and Dice metrics\n",
    "overlap_measures_filter = sitk.LabelOverlapMeasuresImageFilter()\n",
    "overlap_measures_filter.Execute( seg_confidenceConnected, seg_connectedThreshold )\n",
    "print( overlap_measures_filter.GetJaccardCoefficient() )\n",
    "print( overlap_measures_filter.GetDiceCoefficient() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TNR an\n",
    "\n",
    "print( overlap_measures_filter.GetFalseNegativeError() )\n",
    "print( overlap_measures_filter.GetFalsePositiveError() )\n",
    "print( overlap_measures_filter.GetUnionOverlap())\n",
    "print( overlap_measures_filter.GetMeanOverlap())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Jaccard and Dice metrics\n",
    "overlap_measures_filter = sitk.LabelOverlapMeasuresImageFilter()\n",
    "overlap_measures_filter.Execute( seg_connectedThreshold, seg_confidenceConnected  )\n",
    "print( overlap_measures_filter.GetJaccardCoefficient() )\n",
    "print( overlap_measures_filter.GetDiceCoefficient() )\n",
    "\n",
    "print( overlap_measures_filter.GetFalseNegativeError() )\n",
    "print( overlap_measures_filter.GetFalsePositiveError() )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Volume Similarity\n",
    "\n",
    "The volume similarity coefficient, $VS$, is a measure of the similarity between the source and target volumes. Although this measure does not reflect segmentation (or registration, as we will see later in the course) accuracy (i.e. source and target regions can be disjoint and still have qual volumes), it is a conventional measure included for retrospective evaluation of prior studies. It is equal to the differences between two volumes (or areas in 2D) divided by their mean volume (area):\n",
    "\n",
    "$VS = \\frac{| S | - | T |}{| S | + | T |}$\n",
    "\n",
    "(There are many definitions for VS: as an [example](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4533825/pdf/12880_2015_Article_68.pdf). The definition we use is what is implemented in [ITK/SimpleITK](http://www.insight-journal.org/browse/publication/707)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(overlap_measures_filter.GetVolumeSimilarity())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: What does negative VS mean?\n",
    "\n",
    "**Question**: does the input order to sitk.LabelOverlapMeasuresImageFilter() matter for VS?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## enter your codes to answer the above questions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hausdorff Distance\n",
    "\n",
    "Housdorff distance (HD) is a spatial distance based metric widely used in the evaluation of image segmentation as a *dissimilarity measure*. Used in conjunction to overlapping metrics, spatial distance based metrics are recommended when the segmentation overall accuracy, e.e. contour, of the segmentation is of importance.\n",
    "\n",
    "Suppose we got two curves/contours\n",
    "\n",
    "<img src=\"Hausdorff.png\" width=\"550\"/>\n",
    "\n",
    "where $R_n$ is a point on the red curve and $G_n$ is a point on the green curve, respectively. Then the Hausdorff distance between the point sets $R$ and $G$ is defined by:\n",
    "\n",
    "$HD(G,R) = max( h(G,R), h(R,G) )$\n",
    "\n",
    "where $h(g,r)$ is called the *directed* Hausdorff distance and is given by:\n",
    "\n",
    "$h(g,r) = \\max_{g\\in G} \\min_{r \\in R} \\| g - r \\|$\n",
    "\n",
    "where $\\| g - r \\|$ is some distance norm (e.g. Euclidean distance)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this particular example,\n",
    "\n",
    "$HD(G,R) = \\|G_4 - R_4 \\|$.  **WHY**?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## work it out by a simple enumeration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: does $h(G,R)=h(R,G)$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hausdorff distance gives us another perspective to the **shape** of the segmentation. If two segmentations are similar in shape and in proximity, their HD will be small. Using the above example where one label is entirely inside the other, The Dice Coefficient will be the same but the Hausdorff distance will be different depending on the location of the label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hausdorff_distance_filter = sitk.HausdorffDistanceImageFilter()\n",
    "hausdorff_distance_filter.Execute(seg_confidenceConnected, seg_connectedThreshold)\n",
    "print(hausdorff_distance_filter.GetHausdorffDistance())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: does the input order to  sitk.LabelOverlapMeasuresImageFilter matter for Hausdorff Distance?\n",
    "\n",
    "**Question**: What is the unit here?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## enter your code here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segmented_distance_map = sitk.Abs(sitk.SignedMaurerDistanceMap(seg_connectedThreshold, squaredDistance=False))\n",
    "segmented_surface = sitk.LabelContour(seg_connectedThreshold)\n",
    "myshow(segmented_distance_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility method for display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_with_overlay(segmentation_number, slice_number, image, segs, window_min, window_max):\n",
    "    \"\"\"\n",
    "    Display a CT slice with segmented contours overlaid onto it. The contours are the edges of \n",
    "    the labeled regions.\n",
    "    \"\"\"\n",
    "    img = image[:,:,slice_number]\n",
    "    msk = segs[segmentation_number][:,:,slice_number]\n",
    "    overlay_img = sitk.LabelMapContourOverlay(sitk.Cast(msk, sitk.sitkLabelUInt8), \n",
    "                                              sitk.Cast(sitk.IntensityWindowing(img,\n",
    "                                                                                windowMinimum=window_min, \n",
    "                                                                                windowMaximum=window_max), \n",
    "                                                        sitk.sitkUInt8), \n",
    "                                             opacity = 1, \n",
    "                                             contourThickness=[2,2])\n",
    "    #We assume the original slice is isotropic, otherwise the display would be distorted \n",
    "    plt.imshow(sitk.GetArrayViewFromImage(overlay_img))\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetch the data\n",
    "\n",
    "Retrieve a single CT scan and three manual delineations of a liver tumor. Visual inspection of the data highlights the variability between experts. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = sitk.ReadImage(fdata(\"liverTumorSegmentations/Patient01Homo.mha\"))\n",
    "segmentation_file_names = [\"liverTumorSegmentations/Patient01Homo_Rad01.mha\", \n",
    "                          \"liverTumorSegmentations/Patient01Homo_Rad02.mha\",\n",
    "                          \"liverTumorSegmentations/Patient01Homo_Rad03.mha\"]\n",
    "                          \n",
    "segmentations = [sitk.ReadImage(fdata(file_name), sitk.sitkUInt8) for file_name in segmentation_file_names]\n",
    "    \n",
    "interact(display_with_overlay, segmentation_number=(0,len(segmentations)-1), \n",
    "         slice_number = (0, image.GetSize()[2]-1), image = fixed(image),\n",
    "         segs = fixed(segmentations), window_min = fixed(-1024), window_max=fixed(976));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the dataset, there are 3 segmentations from 3 radiologists. From around slice 66 to 80 you will see the outline of the tumor being outlined. Between the 3 radiologists there are some variations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Derive a reference\n",
    "\n",
    "There are a variety of ways to derive a reference segmentation from multiple expert inputs. Several options, and there are more, are described in [\"A comparison of ground truth estimation methods\", A. M. Biancardi, A. C. Jirapatnakul, and A. P. Reeves](https://link.springer.com/article/10.1007%2Fs11548-009-0401-3).\n",
    "\n",
    "Two methods available in SimpleITK are **majority vote** and the **STAPLE** algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use majority voting to obtain the reference segmentation. Note that this filter does not resolve ties. In case of \n",
    "# ties, it will assign max_label_value+1 or a user specified label value (labelForUndecidedPixels) to the result. \n",
    "# Before using the results of this filter you will have to check whether there were ties and modify the results to\n",
    "# resolve the ties in a manner that makes sense for your task. The filter implicitly accommodates multiple labels.\n",
    "labelForUndecidedPixels = 10\n",
    "reference_segmentation_majority_vote = sitk.LabelVoting(segmentations, labelForUndecidedPixels)    \n",
    "\n",
    "manual_plus_majority_vote = list(segmentations)  \n",
    "# Append the reference segmentation to the list of manual segmentations\n",
    "manual_plus_majority_vote.append(reference_segmentation_majority_vote)\n",
    "\n",
    "interact(display_with_overlay, segmentation_number=(0,len(manual_plus_majority_vote)-1), \n",
    "         slice_number = (0, image.GetSize()[2]-1), image = fixed(image),\n",
    "         segs = fixed(manual_plus_majority_vote), window_min = fixed(-1024), window_max=fixed(976));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the absence of the ground truth, the [STAPLE algorithm](https://ieeexplore.ieee.org/document/1309714) estimates what the ground truth is based on multiple segmentations. The mathematical foundation is based on the [Expectation-Maximization](https://en.wikipedia.org/wiki/Expectation%E2%80%93maximization_algorithm) (EM) algorithm, which is quite involved and thus not covered in this class. \n",
    "\n",
    "The STAPLE algorithm is an iterative. At each iteration, the ground truth is estimated based on the input. The performance of each input is evaluated against the estimated ground truth in terms of sensitivity and specificity. If a particular input segmentation is evaluated as *good*, i.e. having high sensitivity and specificity, it is weighted more to generate the next estimate of the ground truth.\n",
    "\n",
    "<img src=\"STAPLE.gif\" width=\"650\"/>\n",
    "\n",
    "(image courtesy of the [STAPLE paper by Warfield et al.](https://ieeexplore.ieee.org/document/1309714))\n",
    "\n",
    "The intermediate results are an image of probability, with value of each pixel/voxel representing the probability for that pixel/voxel to be part of the ground truth, and the estimated sensitivity and specificity for each input segmentation.\n",
    "\n",
    "At the end of the algorithm, this probability map is thresholded to provide an estimate of the ground truth segmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the STAPLE algorithm to obtain the reference segmentation. This implementation of the original algorithm\n",
    "# combines a single label from multiple segmentations, the label is user specified. The result of the\n",
    "# filter is the voxel's probability of belonging to the foreground. We then have to threshold the result to obtain\n",
    "# a reference binary segmentation.\n",
    "foregroundValue = 1\n",
    "threshold = 0.95\n",
    "reference_segmentation_STAPLE_probabilities = sitk.STAPLE(segmentations, foregroundValue) \n",
    "# We use the overloaded operator to perform thresholding, another option is to use the BinaryThreshold function.\n",
    "reference_segmentation_STAPLE = reference_segmentation_STAPLE_probabilities > threshold\n",
    "\n",
    "manual_plus_staple = list(segmentations)  \n",
    "# Append the reference segmentation to the list of manual segmentations\n",
    "manual_plus_staple.append(reference_segmentation_STAPLE)\n",
    "\n",
    "interact(display_with_overlay, segmentation_number=(0,len(manual_plus_staple)-1), \n",
    "         slice_number = (0, image.GetSize()[2]-1), image = fixed(image),\n",
    "         segs = fixed(manual_plus_staple), window_min = fixed(-1024), window_max=fixed(976));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate segmentations using the reference\n",
    "\n",
    "Once we derive a reference from our experts input we can compare segmentation results to it.\n",
    "\n",
    "Note that in this notebook we compare the expert segmentations to the reference derived from them. This is not relevant for algorithm evaluation, but it can potentially be used to rank your experts.\n",
    "\n",
    "In this specific implementation we take advantage of the fact that we have a binary segmentation with 1 for foreground and 0 for background."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "\n",
    "# Use enumerations to represent the various evaluation measures\n",
    "class OverlapMeasures(Enum):\n",
    "    jaccard, dice, volume_similarity, false_negative, false_positive = range(5)\n",
    "\n",
    "class SurfaceDistanceMeasures(Enum):\n",
    "    hausdorff_distance, mean_surface_distance, median_surface_distance, std_surface_distance, max_surface_distance = range(5)\n",
    "    \n",
    "# Select which reference we want to use (majority vote or STAPLE)    \n",
    "reference_segmentation = reference_segmentation_STAPLE\n",
    "\n",
    "# Empty numpy arrays to hold the results \n",
    "overlap_results = np.zeros((len(segmentations),len(OverlapMeasures.__members__.items())))  \n",
    "surface_distance_results = np.zeros((len(segmentations),len(SurfaceDistanceMeasures.__members__.items())))  \n",
    "\n",
    "# Compute the evaluation criteria\n",
    "\n",
    "# Note that for the overlap measures filter, because we are dealing with a single label we \n",
    "# use the combined, all labels, evaluation measures without passing a specific label to the methods.\n",
    "overlap_measures_filter = sitk.LabelOverlapMeasuresImageFilter()\n",
    "\n",
    "hausdorff_distance_filter = sitk.HausdorffDistanceImageFilter()\n",
    "\n",
    "# Use the absolute values of the distance map to compute the surface distances (distance map sign, outside or inside \n",
    "# relationship, is irrelevant)\n",
    "label = 1\n",
    "reference_distance_map = sitk.Abs(sitk.SignedMaurerDistanceMap(reference_segmentation, squaredDistance=False))\n",
    "reference_surface = sitk.LabelContour(reference_segmentation)\n",
    "\n",
    "statistics_image_filter = sitk.StatisticsImageFilter()\n",
    "# Get the number of pixels in the reference surface by counting all pixels that are 1.\n",
    "statistics_image_filter.Execute(reference_surface)\n",
    "num_reference_surface_pixels = int(statistics_image_filter.GetSum()) \n",
    "\n",
    "for i, seg in enumerate(segmentations):\n",
    "    # Overlap measures\n",
    "    overlap_measures_filter.Execute(reference_segmentation, seg)\n",
    "    overlap_results[i,OverlapMeasures.jaccard.value] = overlap_measures_filter.GetJaccardCoefficient()\n",
    "    overlap_results[i,OverlapMeasures.dice.value] = overlap_measures_filter.GetDiceCoefficient()\n",
    "    overlap_results[i,OverlapMeasures.volume_similarity.value] = overlap_measures_filter.GetVolumeSimilarity()\n",
    "    overlap_results[i,OverlapMeasures.false_negative.value] = overlap_measures_filter.GetFalseNegativeError()\n",
    "    overlap_results[i,OverlapMeasures.false_positive.value] = overlap_measures_filter.GetFalsePositiveError()\n",
    "    # Hausdorff distance\n",
    "    hausdorff_distance_filter.Execute(reference_segmentation, seg)\n",
    "    surface_distance_results[i,SurfaceDistanceMeasures.hausdorff_distance.value] = hausdorff_distance_filter.GetHausdorffDistance()\n",
    "    # Symmetric surface distance measures\n",
    "    segmented_distance_map = sitk.Abs(sitk.SignedMaurerDistanceMap(seg, squaredDistance=False))\n",
    "    segmented_surface = sitk.LabelContour(seg)\n",
    "        \n",
    "    # Multiply the binary surface segmentations with the distance maps. The resulting distance\n",
    "    # maps contain non-zero values only on the surface (they can also contain zero on the surface)\n",
    "    seg2ref_distance_map = reference_distance_map*sitk.Cast(segmented_surface, sitk.sitkFloat32)\n",
    "    ref2seg_distance_map = segmented_distance_map*sitk.Cast(reference_surface, sitk.sitkFloat32)\n",
    "        \n",
    "    # Get the number of pixels in the reference surface by counting all pixels that are 1.\n",
    "    statistics_image_filter.Execute(segmented_surface)\n",
    "    num_segmented_surface_pixels = int(statistics_image_filter.GetSum())\n",
    "    \n",
    "    # Get all non-zero distances and then add zero distances if required.\n",
    "    seg2ref_distance_map_arr = sitk.GetArrayViewFromImage(seg2ref_distance_map)\n",
    "    seg2ref_distances = list(seg2ref_distance_map_arr[seg2ref_distance_map_arr!=0]) \n",
    "    seg2ref_distances = seg2ref_distances + \\\n",
    "                        list(np.zeros(num_segmented_surface_pixels - len(seg2ref_distances)))\n",
    "    ref2seg_distance_map_arr = sitk.GetArrayViewFromImage(ref2seg_distance_map)\n",
    "    ref2seg_distances = list(ref2seg_distance_map_arr[ref2seg_distance_map_arr!=0]) \n",
    "    ref2seg_distances = ref2seg_distances + \\\n",
    "                        list(np.zeros(num_reference_surface_pixels - len(ref2seg_distances)))\n",
    "        \n",
    "    all_surface_distances = seg2ref_distances + ref2seg_distances\n",
    "    \n",
    "    surface_distance_results[i,SurfaceDistanceMeasures.mean_surface_distance.value] = np.mean(all_surface_distances)\n",
    "    surface_distance_results[i,SurfaceDistanceMeasures.median_surface_distance.value] = np.median(all_surface_distances)\n",
    "    surface_distance_results[i,SurfaceDistanceMeasures.std_surface_distance.value] = np.std(all_surface_distances)\n",
    "    surface_distance_results[i,SurfaceDistanceMeasures.max_surface_distance.value] = np.max(all_surface_distances)\n",
    "    \n",
    "\n",
    "# Print the matrices\n",
    "np.set_printoptions(precision=3)\n",
    "print(overlap_results)\n",
    "print(surface_distance_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from IPython.display import display, HTML \n",
    "\n",
    "# Graft our results matrix into pandas data frames \n",
    "overlap_results_df = pd.DataFrame(data=overlap_results, index = list(range(len(segmentations))), \n",
    "                                  columns=[name for name, _ in OverlapMeasures.__members__.items()]) \n",
    "surface_distance_results_df = pd.DataFrame(data=surface_distance_results, index = list(range(len(segmentations))), \n",
    "                                  columns=[name for name, _ in SurfaceDistanceMeasures.__members__.items()]) \n",
    "\n",
    "# Display the data as HTML tables and graphs\n",
    "display(HTML(overlap_results_df.to_html(float_format=lambda x: '%.3f' % x)))\n",
    "display(HTML(surface_distance_results_df.to_html(float_format=lambda x: '%.3f' % x)))\n",
    "overlap_results_df.plot(kind='bar').legend(bbox_to_anchor=(1.6,0.9))\n",
    "surface_distance_results_df.plot(kind='bar').legend(bbox_to_anchor=(1.6,0.9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The formatting of the table using the default settings is less than ideal \n",
    "print(overlap_results_df.to_latex())\n",
    "\n",
    "# We can improve on this by specifying the table's column format and the float format\n",
    "print(overlap_results_df.to_latex(column_format='ccccccc', float_format=lambda x: '%.3f' % x))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "widgets": {
   "state": {
    "50046c7234184b2891e3a63b8b540df2": {
     "views": [
      {
       "cell_index": 34
      }
     ]
    }
   },
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
