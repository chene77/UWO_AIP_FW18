{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=\"center\">Advanced Image Processing and Analysis</h1>\n",
    "<h3 align=\"center\">ECE 4438B/ECE 9022/ECE 9202B/BIOMED 9519B/BIOPHYS 9519B/CAMI 9519B</h3>\n",
    "<h4 align=\"center\"><a href=\"mailto:echen29@uwo.ca\"> Elvis Chen, PhD, LL</a></h4>\n",
    "<h4 align=\"center\">Day 08, January 29, 2019</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "\n",
    "We have seen a few thresholding-based techniques, all of which takes advantages of certain characteristics of the image histogram. For example, [Otsu's method](https://en.wikipedia.org/wiki/Otsu%27s_method) assumes that the histogram has a bimodal distribution, and computes a threshold value that maximizes the inter-class variance.\n",
    "\n",
    "In this Jupyter Noteobok we will (re)visit the mathematical aspect of these algorithms: namely how to compute these statistical measures and actually implements these algorithms ourselves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import SimpleITK as sitk\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib notebook\n",
    "\n",
    "# utilities for diplaying images\n",
    "from myshow import myshow, myshow3d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RGB to Grayscale\n",
    "\n",
    "But first, we assume the histogram is computed based on a gray-scale image. What if our image is RGB and not gray-scale?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us load a color image and convert it ourselves:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_rgb = sitk.ReadImage('..\\\\data\\\\images\\\\onion.png')\n",
    "#img_rgb = sitk.ReadImage('..\\\\data\\\\images\\\\Lenna.png')\n",
    "sitk.Show(img_rgb, title='Colour Image')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But how do we convert an RGB image to BW?\n",
    "\n",
    "Since RGB picture is nothing but a picture of 3 channels, one naive way is to average three channels together."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Simple Averaging**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_Vector = sitk.GetArrayFromImage( img_rgb ) # copy the pixel to a numpy array\n",
    "print(img_Vector.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a BW image by simple averaging\n",
    "img_bw = sitk.GetImageFromArray((img_Vector[:,:,0] + img_Vector[:,:,1] + img_Vector[:,:,2])/3.0)\n",
    "img_bw.CopyInformation( img_rgb ) # why do we do this?\n",
    "sitk.Show(img_bw, title='Black and White Image')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Does it look right?\n",
    "\n",
    "As it turns out, human vision does not preceive each color equally: [Green light contributes the most to the intensity perceived by human eyes, and blue light the least](https://en.wikipedia.org/wiki/Relative_luminance).\n",
    "\n",
    "**Thus, we need to have an weighted average instead.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_bw = sitk.GetImageFromArray(.299 * img_Vector[:,:,0] + .587 * img_Vector[:,:,1] + .114 * img_Vector[:,:,2])\n",
    "img_bw.CopyInformation( img_rgb ) # why do we do this?\n",
    "sitk.Show(img_bw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That is, the green colour contributes roughly $58.7\\%$, red color contributes $29.9\\%$, and blue colour contributes the least at about $11.4\\%$, and the BW image is what we expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# althought not necessary here, let us do it as a good practice\n",
    "img_255 = sitk.Cast(sitk.RescaleIntensity(img_bw), sitk.sitkUInt8) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### statistical measures\n",
    "\n",
    "Perhaps we should examine the definition of a few metrics and implements them ourselves.\n",
    "\n",
    "If we treat the pixel intensity ($I$) as a [random variable](https://en.wikipedia.org/wiki/Random_variable), "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "I = sitk.GetArrayFromImage(img_bw)\n",
    "print(I.shape)\n",
    "I = I.flatten()\n",
    "print(I.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculating the sum of a sequence\n",
    "sum = 0.0\n",
    "n = I.shape[0]\n",
    "for i in range(0,n):\n",
    "    sum = sum + I[i]\n",
    "    \n",
    "print(sum)\n",
    "\n",
    "## or\n",
    "print(I.sum())\n",
    "print(np.sum(I))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myMean = sum/n\n",
    "print(myMean)\n",
    "\n",
    "# or\n",
    "print(myMean.mean())\n",
    "print(np.mean(I))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Standard deviation\n",
    "\n",
    "Recall the definition of the [**sampled** standar deviation is](https://en.wikipedia.org/wiki/Standard_deviation):\n",
    "\n",
    "\n",
    "$\\sigma = \\sqrt{\\frac{\\sum^{N}_{i=1}(x_i-\\mu)^2}{N-1}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mySigma = np.sqrt(np.sum((I-myMean)*(I-myMean))/(n-1))\n",
    "\n",
    "print(mySigma)\n",
    "\n",
    "# or\n",
    "print(I.std())\n",
    "print(np.std(I))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**WHY THE DIFFERENCE?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us divide by n instead of (n-1)\n",
    "mySigma = np.sqrt(np.sum((I-myMean)*(I-myMean))/(n))\n",
    "print(mySigma)\n",
    "\n",
    "# or\n",
    "print(I.std())\n",
    "print(np.std(I))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Variance\n",
    "\n",
    "Variance is [standard deviation **squared**](https://en.wikipedia.org/wiki/Variance):\n",
    "\n",
    "$\\mu = \\frac{\\sum^{N}_{i=1}(x_i-\\mu)^2}{N-1}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myMu = np.sum((I-myMean)*(I-myMean))/(n-1)\n",
    "\n",
    "print(myMu)\n",
    "\n",
    "# or\n",
    "print(I.var())\n",
    "print(np.var(I))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**AGAIN, WHY THE DIFFERENCE?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us divide by n instead of (n-1)\n",
    "myMu = np.sum((I-myMean)*(I-myMean))/(n)\n",
    "print(myMu)\n",
    "\n",
    "# or\n",
    "print(I.var())\n",
    "print(np.var(I))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### which one \n",
    "is **right**?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = sitk.StatisticsImageFilter()\n",
    "stats.Execute(img_bw)\n",
    "print(\"Standard Deviation:\", stats.GetSigma(), \"Variance:\", stats.GetVariance())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As it turns out, the difference being whether one wishes to compute the standard deviation/variance **exactly** (of a known sequence), or **estimate** it based on a random variable following a [stochastic process](https://en.wikipedia.org/wiki/Stochastic_process).\n",
    "\n",
    "In Python, these values are computed exactly (i.e. divided by $(n)$).\n",
    "\n",
    "But since we are working with images, and we assumes that the pixel intensity follows a stochastic process, the standard deviation/variance computed by **division of $(n)$** is [*biased*](https://en.wikipedia.org/wiki/Bias_of_an_estimator): Dividing instead by $(n-1)$ yields an unbiased estimator.\n",
    "\n",
    "In ITK/SimpleITK and also in **MATLAB**, we use the **unbiased** estimator instead."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "% Matlab code sample\n",
    "n = 50;\n",
    "I = 100 * randn([1,n]); % create 50 randomly generated variable following a gaussian distribution and scale by 100\n",
    "myVariance1 = sum((I-mean(I)).*(I-mean(I)))/(n)\n",
    "myVariance2 = sum((I-mean(I)).*(I-mean(I)))/(n-1)\n",
    "var(I)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the sample size $n$ is large, then the difference between division by $(n)$ and $(n-1)$ is small and perhaps makes no difference in practice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other definitions\n",
    "\n",
    "The simplest form of segmentation is to find a thresholding value that separates the **foreground** from the **background**.  Historically, we can denote the background as **b** or $0$, and foreground as **w** or $1$.  For a given image $I$, we can compute the probability of pixel intensity $p_i$, as defined by the histogram divided by the total number of pixels.\n",
    "\n",
    "Let the pixels of a given pixture be represented in $L$ gray levels $[0, 1, ..., L-1]$. In a typical BW image, we will assume $L=256$ for the purpose of this Jupyter Notebook, although for a CT or MRI image, the mimimum/maximum pixel intensity will vary.\n",
    "\n",
    "\n",
    "#### Histogram\n",
    "The number of pixels at the intensity level $i$ is denoted by $n_i$, and the total number of pixels is denoted by $N$:\n",
    "\n",
    "$N = n_0 + n_1 + ... + n_{L-1}$\n",
    "\n",
    "In another words, $N$ is the histogram of the image with $L$-bins.\n",
    "\n",
    "\n",
    "#### normalized Histogram\n",
    "Define the probability of each pixel intensity $p_i$ as:\n",
    "\n",
    "$p_i = \\frac{n_i}{N}$ \n",
    "\n",
    "and note that:\n",
    "\n",
    "$p_i \\ge 0$ \n",
    "\n",
    "and \n",
    "\n",
    "$\\sum_{i=0}^{L-1} p_i = 1$\n",
    "\n",
    "That is, $p_i$ is the normalized histogram."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### cumulative density function\n",
    "\n",
    "For a given threshold value $t$, we are separating these pixels into two class: those with an intensity value $\\lt t$ is grouped (labelled) as the class **black** (or $0$), and those with intensity value $\\ge t$ is grouped (labelled) as the class **white** (or $1$). The class probability $\\omega_{0,1}(t)$ is computed from the $L$ bins of the normalized histogram:\n",
    "\n",
    "$\\omega_0(t) = \\sum_{i=0}^{t-1} p_i$\n",
    "\n",
    "$\\omega_1(t) = \\sum_{i=t}^{L-1} p_i$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That is, $\\omega_w(t)$ is the [**cumulative density function**](https://en.wikipedia.org/wiki/Cumulative_distribution_function) of the histogram **up to** $t$. Note:\n",
    "\n",
    "$\\omega_0(t) + \\omega_1(t) = 1$\n",
    "\n",
    "by definition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[**As a side note**](https://en.wikipedia.org/wiki/Moment_(mathematics%29), $w_0$ is also known as the zeroth-order cumulative moment of the histogram up to the $k^{th}$ level. It represents the area under the curve: <img src=\"areaUnderCurve.png\" width=\"550\"/> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### the Class mean (average) \n",
    "\n",
    "The **mean**, or the **average**, pixel intensity for each class can be computed:\n",
    "\n",
    "$\\mu_0(t) = \\sum_{i=0}^{t-1} i p_i$\n",
    "\n",
    "\n",
    "$\\mu_1(t) = \\sum_{i=t}^{L-1} i p_i$\n",
    "\n",
    "\n",
    "and the mean pixel intensity for the entire image is:\n",
    "\n",
    "$\\mu_T \\ = \\sum_{i=0}^{L-1} i p_i$ \n",
    "\n",
    "and we can easily varify that:\n",
    "\n",
    "$\\omega_0 \\mu_0 + \\omega_1 \\mu_1 = \\mu_T$\n",
    "\n",
    "[**As a side note**](https://en.wikipedia.org/wiki/Moment_(mathematics%29), $\\mu_0$ is also known as the first-order cumulative moment of the histogram up to the $k^{th}$ level."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Otsu's method\n",
    "\n",
    "**According to Wikipedia**, in [Otsu's method](https://en.wikipedia.org/wiki/Otsu%27s_method), one exhaustively search for a threshold value $t$ that\n",
    "* minimizes the intra-class variance (the variance within the class), defined as the weighted sum of the variances of the two classes:\n",
    "\n",
    "$\\sigma^2_{w}(t) = \\omega_{0}(t) \\sigma^2_{0}(t) + \\omega_{1}(t) \\sigma_{1}^{2}(t)$\n",
    "\n",
    "where the weights $\\omega_{0}$ and $\\omega_{1}$ are the probabilities of the two classes separated by a threhold $t$, and $\\sigma_{0}^{2}$ and $\\sigma_{1}^{2}$ are variances of these two classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, one may implement it naively.  First we can compute the histogram manually, assuming this is a BW image with $256$ gray scales (bins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = np.histogram( img_bw, bins=256, range=[0,256] )\n",
    "print(n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The histogram computed using numpy contains 2 arrays: the first is the frequency, the second is the bin:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting the histogram manually\n",
    "plt.figure() # use the figure() commmand to create a NEW figure\n",
    "plt.plot(n[1][1:],n[0]) # note the size difference\n",
    "plt.xlabel('Pixel Intensity')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Histogram')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For efficiency, we can compute the **normalized** histogram:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = 1.0/np.sum(n[0]) * n[0]\n",
    "# bw_histogram_normalized = 1.0/(bw_histogram[0].sum()) * bw_histogram[0]\n",
    "plt.figure()\n",
    "plt.plot(n[1][1:], p)\n",
    "plt.xlabel('Pixel Intensity')\n",
    "plt.ylabel('Probability')\n",
    "plt.title('Probability Density Function')\n",
    "\n",
    "# How is this plot different from the previous one?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also compute the cumulative distribution function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CDF = np.zeros( len( p ) )\n",
    "CDF[0] = p[0]\n",
    "for i in range(1, len(p )):\n",
    "    CDF[i] = CDF[i-1] + p[i]\n",
    "        \n",
    "plt.figure()\n",
    "plt.plot(n[1][1:], CDF )\n",
    "plt.xlabel('Pixel Intensity')\n",
    "plt.ylabel('Probability')\n",
    "plt.title('Cumulative Density Function')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hence, we can define the following variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_0 = CDF\n",
    "w_1 = 1-w_0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the class mean:\n",
    "\n",
    "$\\mu_0 = \\sum_{i=0}^{t-1} i \\frac{p_i}{\\omega_0} = \\frac{\\mu(k)}{\\omega(k)}$\n",
    "\n",
    "\n",
    "$\\mu_1 = \\sum_{i=t}^{L-1} i \\frac{p_i}{\\omega_1} = \\frac{\\mu_T - \\mu(k)}{1-\\omega(k)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and the class variances are given by:\n",
    "\n",
    "$\\sigma^2_0 = \\sum_{i=1}^{t} (i-\\mu_0)^2 \\frac{p_i}{\\omega_0}$\n",
    "\n",
    "$\\sigma^2_1 = \\sum_{i=t+1}^{L} (i-\\mu_1)^2 \\frac{p_i}{\\omega_1}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Take a close look at these definitions.  What happens when the histogram bin is $0$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As it turns out, the intra-class variance:\n",
    "\n",
    "\n",
    "$\\sigma^2_{w}(t) = \\omega_{0}(t) \\sigma^2_{0}(t) + \\omega_{1}(t) \\sigma_{1}^{2}(t)$\n",
    "\n",
    "is **not** the only functional we can optimize.  Refer to the [original paper by Otsu](http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=4310076): Equation (12) to (10) states that, the minimization of the intra-class variance is the same as maximization of the inter-class variance $\\sigma_b^2$:\n",
    "\n",
    "$\\sigma^2_b(t) = \\frac{[\\mu_T \\omega(t) - \\mu(t)]^2}{\\omega_0(k) (1-\\omega_0(k)) }$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which can be easily implemented. Note that the numerator is always **positive**. Hence, if the $\\omega_0{k} == 0$ or $\\omega_1{k}==0$, we can simply set $\\sigma_b^2(t)$ to $0$ instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# P is the normalized histogram\n",
    "\n",
    "levels = np.zeros([256])\n",
    "w = np.zeros([256])     # 0th-order cumulative moment, or CDF, we computed it earlier but repeating it for completeness\n",
    "mu = np.zeros([256])    # 1st-order cumulative moment, or the class mean\n",
    "w[0] = p[0]\n",
    "mu[0] = 0\n",
    "\n",
    "# preparation\n",
    "for t in range(1,256):    # for each threshold value t, compute the 0th and 1st moment\n",
    "    w[t] = w[t-1] + p[t]\n",
    "    mu[t] = mu[t-1] + t * p[t]\n",
    "mu_T = mu[255]            # the total mean pixel intensity of the original image\n",
    "\n",
    "\n",
    "# exhustive search\n",
    "for t in range(0,256):\n",
    "    if ( w[t]*(1-w[t]) > 0 ): # avoid division by 0\n",
    "        levels[t] = ((mu_T * w[t] - mu[t])*(mu_T * w[t] - mu[t])) /(w[t]*(1-w[t]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(range(0,256), levels)\n",
    "plt.xlabel('Pixel Intensity')\n",
    "plt.ylabel('variance')\n",
    "plt.title('Intra-class variance')\n",
    "print(np.argmax(levels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let s verify our implementation against SimpleITK's implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_255 = sitk.Cast(sitk.RescaleIntensity(img_bw), sitk.sitkUInt8)\n",
    "otsu_filter = sitk.OtsuThresholdImageFilter()\n",
    "inside_value = 1\n",
    "outside_value = 0\n",
    "otsu_filter.SetInsideValue(inside_value)\n",
    "otsu_filter.SetOutsideValue(outside_value)\n",
    "seg = otsu_filter.Execute(img_bw)\n",
    "sitk.Show(sitk.LabelOverlay(img_255, seg), title='Segmented Image') # the image is too large to use myshow\n",
    "print( otsu_filter.GetThreshold())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alternative (more efficient) implementation\n",
    "\n",
    "As it turned out, we needed 2 for-loops for our computation. It is conceptually clean but slightly inefficient.  Specifically, we needed the 1st for-loop to compute the total mean leavel of the original image. \n",
    "\n",
    "**as a side note**, here is an alternative implementation that requires only 1 for-loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "histogramCounts = n[0]\n",
    "levels = np.zeros([256])\n",
    "N = histogramCounts.sum()\n",
    "sumB = 0\n",
    "wB = 0\n",
    "maximum = 0.0\n",
    "sum1 = np.sum( range(0,256) * histogramCounts )\n",
    "level = 0\n",
    "for ii in range(0,256):\n",
    "    wB = wB + histogramCounts[ii]\n",
    "    wF = N - wB\n",
    "    if ( wB !=0 and wF != 0):\n",
    "        sumB = sumB + ii * histogramCounts[ii]\n",
    "        mF = ( sum1 - sumB ) / wF\n",
    "        between = wB * wF * (( sumB / wB ) - mF ) * ((sumB/wB) - mF)\n",
    "        levels[ii] = between\n",
    "        if ( between >= maximum ):\n",
    "            level = ii\n",
    "            maximum = between\n",
    "print(level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(range(0,256), levels)\n",
    "plt.xlabel('Pixel Intensity')\n",
    "plt.ylabel('variance')\n",
    "plt.title('Intra-class variance')\n",
    "print(np.argmax(levels))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
